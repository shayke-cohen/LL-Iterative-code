
Detailed Requirement Document: Iterative Code Generation with LLMs

Project Overview
----------------
This project aims to develop an iterative code generation system using a Language Model (LLM). The system will generate, refine, and validate code iteratively by integrating various development tools, including compilers, linters, testing frameworks, static analysis tools, runtime log collectors, and package management tools. The process will continue until the LLM confirms that the task is complete, or a maximum number of iterations is reached.

The LLM's output will be in JSON format, encapsulating updated files, tool actions, summaries, and a history of actions, ensuring that the LLM does not repeat actions unnecessarily.

Functional Requirements
------------------------
1. Task Initialization

- Input: A task description, relevant files, working files, and the project root directory.
  - Task Description: A clear, concise description of the programming task (e.g., "Fix a bug in the login feature and improve performance").
  - Relevant Files: A list of files related to the task (e.g., `login.js`, `auth.js`, `index.ts`).
  - Working Files: A subset of relevant files actively being worked on in the current iteration.
  - Project Root Directory: The base directory for all file paths and the working directory for all tools.
  - **Enable Questions:** An option to allow the LLM to ask questions during the code generation process.

- Output: Initialization of the iterative code generation loop.


2. Iterative Code Generation Loop

The system should repeatedly generate, refine, and validate code based on the task and relevant files. Each iteration consists of the following steps:

2.1 Code Generation/Update
- Input:
  - Task description
  - Relevant files
  - Working files
  - Project root directory
  - Results from tools run in previous iterations (if any)
  - History of actions from previous iterations
  - Enable Questions: If set, the LLM can output questions that need to be answered in the next iteration.

- Process:
  - The LLM generates or updates code based on the input.
  - Modify or create new files as needed.

- Output: JSON object containing:
  - Updated Files: Files that have been changed or newly created.
  - Actions: A summary of actions taken (e.g., code generated, file updated).
  - History: A list of actions from previous iterations, with date/time stamps.
  - **Questions:** If enabled, the LLM may include questions that require answers for the next iteration.

2.2 Running Tools
- Input: Working files with updated code and the project root directory.

- Tools to be Run:
  - TypeScript Compiler (`tsc`): Ensure code validity and type safety.
  - Jest: Run unit tests to verify functionality.
  - ESLint: Lint the code for style and potential errors.
  - SonarQube: Perform static analysis to find bugs, vulnerabilities, and code smells.
  - Runtime Logs: Collect runtime logs if the LLM suggests executing the code.
  - CI/CD Pipeline: Optionally, run a CI/CD pipeline to test the code in a production-like environment.
  - Dependency Checker: Run tools like `npm audit` or `Snyk` to check for outdated or vulnerable dependencies.
  - File Operations: Tools for moving files and deleting files as suggested by the LLM.
  - Yarn Operations:
    - yarn install: Install dependencies.
    - yarn build: Build the project.
    - yarn test: Run tests.
    - remove node_modules: Clean up the `node_modules` directory.
  - **Question Answering Tool:** A tool for capturing and processing the userâ€™s answers to the questions generated by the LLM.

- Output: JSON object containing:
  - Tool Results: Results from each tool, including pass/fail status, error logs, warnings, etc.
  - Actions: A summary of the changes or operations performed by each tool.



2.3 LLM Analysis
- Input:
  - Task description
  - Relevant files
  - Working files
  - Project root directory
  - Results from tools
  - History of actions from previous iterations
  - Enable Questions: If set, the LLM can output questions that need to be answered in the next iteration.
  - Questions: Any questions asked by the LLM that the user needs to answer.

- Process:
  - The LLM analyzes the tool results and decides whether the task is complete.
  - If the task is not complete, the LLM updates the task description and/or the working files for the next iteration.
  - If questions are enabled, the LLM may generate questions that need answers in the next iteration.

- Output: JSON object containing:
  - Task Status: Whether the task is complete or needs further iterations.
  - Updated Task: Any updates to the task description, if applicable.
  - Updated Files: Any changes to the working files.
  - Actions: A summary of actions taken, including tool results and file operations.
  - History: An updated history of actions, including the current iteration's date/time and actions.
  - Questions: If questions are enabled, the LLM may generate questions.

2.4 Iteration Control
- Input:
  - Current iteration count
  - LLM Analysis Output

- Process:
  - Check if the maximum number of iterations (`maxIterations`) has been reached.
  - If the task is complete or the maximum iterations have been reached, exit the loop.
  - Otherwise, proceed to the next iteration.

- Output: Either continue to the next iteration or exit the loop.

2.5 Logging
- Input: All data from the current iteration.
- Process: Log the results of each iteration, including LLM responses, tool outputs, and any manual feedback.
- Output: Detailed logs for post-mortem analysis or debugging.



3. Handling Edge Cases
- Max Iterations Reached: If the maximum number of iterations is reached, notify the user and log the situation.
- Tool Failures: If a critical tool fails (e.g., `tsc` returns an error), log the issue and allow the LLM to decide on the next steps.
- Human-in-the-Loop: At predefined checkpoints, allow a developer to review the iteration results and provide feedback.

Non-Functional Requirements
----------------------------
1. Performance
- The system should handle iterations efficiently, with parallel execution of independent tools where possible.
- Ensure that the process does not exceed predefined resource limits (e.g., memory, CPU).

2. Scalability
- The system should be scalable to handle projects with a large number of files and complex tasks.

3. Reliability
- Ensure that all iterations are logged with sufficient detail to allow for debugging.
- Implement automated rollback mechanisms to revert to the last known good state if necessary.

4. Maintainability
- The system should be modular, allowing for easy updates or the addition of new tools and LLM models.
- Code should be well-documented and follow standard best practices.

5. Security
- All code and data should be handled securely, with proper access controls and vulnerability scanning for dependencies.

LLM Prompts
-----------
1. Code Generation/Update Prompt
- The LLM should generate or update code based on the given inputs and avoid repeating actions unnecessarily.

Example Prompt for Code Generation:

{
  "task": "{task description}",
  "relevantFiles": [
    {"fileName": "file1.js", "contentSnippet": "function example() { ... }"},
    {"fileName": "file2.ts", "contentSnippet": "class Example { ... }"}
  ],
  "workingFiles": [
    {"fileName": "workingFile1.js", "contentSnippet": "let x = 10; ..."}
  ],
  "projectRootDirectory": "{/path/to/project/root}",
  "previousToolResults": {
    "tsc": "No errors",
    "jest": "2 tests failed",
    "eslint": "3 warnings"
  },
  "history": [
    {"iteration": 1, "action": "Generated new function in file1.js", "timestamp": "2024-08-25T10:00:00Z"},
    {"iteration": 2, "action": "Fixed test cases in workingFile1.js", "timestamp": "2024-08-25T10:10:00Z"}
  ],
  "enableQuestions": true,  # Enable the LLM to ask questions
  "questions": []  # This will be populated in future iterations with user responses
}

2. Tool Results Analysis Prompt
- The LLM analyzes tool results, updates the task description if necessary, and ensures actions aren't redundant.

Example Prompt for Tool Results Analysis:

{
  "task": "{task description}",
  "relevantFiles": [
    {"fileName": "file1.js", "contentSnippet": "function example() { ... }"},
    {"fileName": "file2.ts", "contentSnippet": "class Example { ... }"}
  ],
  "workingFiles": [
    {"fileName": "workingFile1.js", "contentSnippet": "let x = 10; ..."}
  ],
  "projectRootDirectory": "{/path/to/project/root}",
  "toolResults": {
    "tsc": "No errors",
    "jest": "All tests passed",
    "eslint": "No warnings",
    "sonarqube": "No vulnerabilities found",
    "runtimeLogs": "Function executed successfully"
  },
  "history": [
    {"iteration": 1, "action": "Generated new function in file1.js", "timestamp": "2024-08-25T10:00:00Z"},
    {"iteration": 2, "action": "Fixed test cases in workingFile1.js", "timestamp": "2024-08-25T10:10:00Z"},
    {"iteration": 3, "action": "Ran tests and resolved ESLint warnings", "timestamp": "2024-08-25T10:20:00Z"}
  ],
  "enableQuestions": true,
  "questions": [{"question": "Do you prefer option A or option B?", "answer": ""}]
}



Pseudocode
----------
The following pseudocode summarizes the iterative code generation process with the new features for asking questions, requesting additional files, and maintaining a task history:

function iterativeCodeGeneration(task, relevantFiles, workingFiles, projectRootDirectory, enableQuestions, enableFileRequest):
    iterationCount = 0
    maxIterations = 10  # Set a maximum number of iterations to prevent infinite loops
    questions = []  # Initialize an empty list to store questions
    additionalFiles = []  # Initialize an empty list to store additional files requested by the LLM
    taskHistory = []  # Initialize a list to store the task history

    loop:
        iterationCount += 1
        print(f"Starting iteration {iterationCount}")

        # Step 1: Generate code or update files based on the task, relevant files, tool results, and project root directory
        llmPrompt = generateLLMPrompt(task, relevantFiles, workingFiles, toolResults, projectRootDirectory, enableQuestions, questions, enableFileRequest, additionalFiles)

        # Step 2: Execute the tools
        toolResults = runTools(llmPrompt["workingFiles"], projectRootDirectory)

        # Step 3: Analyze the results
        analysis = analyzeResults(toolResults, llmPrompt["task"], llmPrompt["history"])

        # Step 4: Handle questions and file requests if enabled
        if enableQuestions and "questions" in analysis:
            questions = analysis["questions"]
        if enableFileRequest and "fileRequest" in analysis:
            additionalFiles = handleFileRequest(analysis["fileRequest"])

        # Step 5: Check iteration control
        if analysis["taskStatus"] == "complete" or iterationCount >= maxIterations:
            break

        # Step 6: Prepare for the next iteration
        task = analysis["updatedTask"]
        taskHistory.append(task)  # Add the updated task to the task history
        workingFiles = analysis["updatedFiles"]
        history.append({"iteration": iterationCount, "actions": analysis["actions"], "timestamp": getCurrentTimestamp()})

    return {"finalTaskStatus": analysis["taskStatus"], "history": history, "questions": questions, "fileRequest": additionalFiles, "taskHistory": taskHistory}



Sample LLM Flows
----------------
This section provides sample flows of how the LLM can interact with the system across multiple iterations. The flows demonstrate how the system handles task generation, tool execution, question handling, file requests, and task history management.

Flow 1: Simple Code Generation
------------------------------
1. Task: "Implement a new function in file1.js"
2. Relevant Files: file1.js
3. LLM generates the new function code.
4. Tools run (e.g., `tsc`, `eslint`) to validate the code.
5. The task is marked as complete.

Flow 2: Iterative Development with Questions
--------------------------------------------
1. Task: "Fix a bug in file1.js"
2. Relevant Files: file1.js
3. LLM attempts to fix the bug but needs clarification.
4. LLM asks: "Should we log the error or handle it silently?"
5. User provides an answer, and the LLM adjusts the code accordingly.
6. Tools run to validate the code.
7. The task is updated and recorded in the task history.

Flow 3: File Request Handling
-----------------------------
1. Task: "Optimize performance in file1.js"
2. Relevant Files: file1.js
3. LLM realizes additional context is needed from `utils.js`.
4. LLM requests the `utils.js` file.
5. User provides the file, and the LLM continues with the optimization.
6. Tools run, and the task history is updated.



Code Generation/Update Prompt
-----------------------------
Example Prompt for Code Generation:
{
  "task": "{task description}",
  "relevantFiles": [
    {"fileName": "file1.js", "contentSnippet": "function example() { ... }"},
    {"fileName": "file2.ts", "contentSnippet": "class Example { ... }"}
  ],
  "workingFiles": [
    {"fileName": "workingFile1.js", "contentSnippet": "let x = 10; ..."}
  ],
  "projectRootDirectory": "{/path/to/project/root}",
  "previousToolResults": {
    "tsc": "No errors",
    "jest": "2 tests failed",
    "eslint": "3 warnings"
  },
  "history": [
    {"iteration": 1, "action": "Generated new function in file1.js", "timestamp": "2024-08-25T10:00:00Z"},
    {"iteration": 2, "action": "Fixed test cases in workingFile1.js", "timestamp": "2024-08-25T10:10:00Z"}
  ],
  "enableQuestions": true,  # Enable the LLM to ask questions
  "questions": []  # This will be populated in future iterations with user responses
}

Example Prompt for Tool Results Analysis:
{
  "task": "{task description}",
  "relevantFiles": [
    {"fileName": "file1.js", "contentSnippet": "function example() { ... }"},
    {"fileName": "file2.ts", "contentSnippet": "class Example { ... }"}
  ],
  "workingFiles": [
    {"fileName": "workingFile1.js", "contentSnippet": "let x = 10; ..."}
  ],
  "projectRootDirectory": "{/path/to/project/root}",
  "toolResults": {
    "tsc": "No errors",
    "jest": "All tests passed",
    "eslint": "No warnings",
    "sonarqube": "No vulnerabilities found",
    "runtimeLogs": "Function executed successfully"
  },
  "history": [
    {"iteration": 1, "action": "Generated new function in file1.js", "timestamp": "2024-08-25T10:00:00Z"},
    {"iteration": 2, "action": "Fixed test cases in workingFile1.js", "timestamp": "2024-08-25T10:10:00Z"},
    {"iteration": 3, "action": "Ran tests and resolved ESLint warnings", "timestamp": "2024-08-25T10:20:00Z"}
  ],
  "enableQuestions": true,
  "questions": [{"question": "Do you prefer option A or option B?", "answer": ""}]
}



